{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b248531-9240-4cc6-b7e6-aa782234f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ca4bc9-ce95-45e0-b6c7-b70a9134e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8880d-a4e8-4345-bfe7-8eafcd5ad7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Core Components of the Hadoop Ecosystem\n",
    "HDFS (Hadoop Distributed File System): A distributed file system that stores big data across multiple nodes by splitting large files into blocks. It ensures fault tolerance and high throughput.\n",
    "\n",
    "MapReduce: A programming model used for processing large data sets in parallel. It consists of two phases—Map (which processes input data) and Reduce (which aggregates intermediate data).\n",
    "\n",
    "YARN (Yet Another Resource Negotiator): Manages and schedules resources across the Hadoop cluster. It handles task distribution and resource allocation efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b0735-887f-4a1b-aa84-a0297e32975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Hadoop Distributed File System (HDFS) Details\n",
    "NameNode: The master node that stores metadata (e.g., file locations, permissions) and manages DataNodes.\n",
    "\n",
    "DataNode: Slave nodes that store actual data in blocks. DataNodes report to the NameNode.\n",
    "\n",
    "Blocks: HDFS splits large files into 128 MB blocks (default) which are distributed across DataNodes. Each block is replicated (default: 3 replicas) to ensure fault tolerance. If one node fails, data can be retrieved from the replicated block.\n",
    "\n",
    "HDFS provides fault tolerance via replication and high availability using secondary NameNodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b70f8-103d-4c07-b2c1-12dfbb865e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. MapReduce Framework Explained\n",
    "Step-by-Step Working:\n",
    "\n",
    "Input Splitting: The data is split into smaller chunks.\n",
    "Map Phase: Each split of data is processed in parallel by mapper tasks. A real-world example: In counting word frequency from text, each map function processes a part of the text and outputs key-value pairs like <word, 1>.\n",
    "Shuffle and Sort: The intermediate key-value pairs are shuffled and sorted based on the key.\n",
    "Reduce Phase: Reducer tasks aggregate the mapped data, e.g., summing up the values for each key to get <word, total_count>.\n",
    "Output: The reduced output is written back to HDFS.\n",
    "Advantages:\n",
    "\n",
    "Processes large datasets in parallel.\n",
    "Handles fault tolerance with automatic re-execution.\n",
    "Limitations:\n",
    "\n",
    "High latency due to disk-based storage between the map and reduce phases.\n",
    "Not ideal for iterative processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efe0c1-7092-42f8-b7cf-a719dc992324",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. YARN’s Role in Hadoop\n",
    "YARN decouples the resource management and job scheduling/monitoring from the MapReduce process in Hadoop. It consists of two key components:\n",
    "\n",
    "ResourceManager: Allocates resources to various applications.\n",
    "NodeManager: Manages individual nodes and monitors resource usage.\n",
    "Benefits over Hadoop 1.x:\n",
    "\n",
    "Supports multiple applications beyond MapReduce.\n",
    "Improves scalability by efficiently using cluster resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5656a24-8a19-45d2-b38b-c7bea14b34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Popular Components in the Hadoop Ecosystem\n",
    "HBase: A NoSQL database that provides real-time read/write access to large datasets.\n",
    "Hive: Data warehouse infrastructure built on top of Hadoop for querying large datasets using SQL-like language (HiveQL).\n",
    "Pig: A platform for analyzing large data sets using a high-level scripting language (Pig Latin).\n",
    "Spark: A fast, in-memory data processing engine that works with Hadoop's data storage systems.\n",
    "Example: Hive can be integrated into the Hadoop ecosystem for querying structured data stored in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2656e-6c04-4bb7-8fe3-1ca463e672cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Apache Spark vs Hadoop MapReduce\n",
    "In-Memory Processing: Spark processes data in memory, making it faster than MapReduce, which writes intermediate results to disk.\n",
    "Ease of Use: Spark provides high-level APIs in Java, Scala, Python, and R, whereas MapReduce requires low-level Java programming.\n",
    "Iterative Processing: Spark excels in iterative algorithms (e.g., machine learning), where data needs to be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e1874-5fa5-4250-9626-fd9ba684a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Spark Application for Word Count (Python Example)\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Word Count\")\n",
    "\n",
    "text_file = sc.textFile(\"file.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split()) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "top_10 = counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091c045-a416-46c7-bc8d-892d8ae319b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatMap: Splits the text into words.\n",
    "map: Maps each word to the count of 1.\n",
    "reduceByKey: Aggregates the word counts.\n",
    "takeOrdered: Returns the top 10 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd34c2-a934-4dd2-93e9-de59f0fbefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Working with Spark RDDs\n",
    "\n",
    "#Filter:\n",
    "filtered_rdd = rdd.filter(lambda x: x['age'] > 30)\n",
    "\n",
    "#Map:\n",
    "mapped_rdd = rdd.map(lambda x: (x['name'], x['salary'] * 1.1))\n",
    "\n",
    "#Reduce:\n",
    "sum_salary = rdd.map(lambda x: x['salary']).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfd978-b6bb-4938-a3b0-f84c8718557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Spark DataFrame Operations (Python Example)\n",
    "\n",
    "#Loading DataFrame:\n",
    "df = spark.read.csv('file.csv', header=True, inferSchema=True)\n",
    "\n",
    "#Select Specific Columns:\n",
    "df.select('name', 'age').show()\n",
    "\n",
    "#Filter Rows:\n",
    "df.filter(df['age'] > 30).show()\n",
    "\n",
    "#Group By and Aggregation:\n",
    "df.groupBy('department').agg({'salary': 'avg'}).show()\n",
    "\n",
    "#Join:\n",
    "df1.join(df2, df1['id'] == df2['id'], 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab13a4-5faf-487b-869d-82bf5c5d0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Spark Streaming Application Example\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Streaming\").getOrCreate()\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "counts = lines.flatMap(lambda line: line.split()) \\\n",
    "              .map(lambda word: (word, 1)) \\\n",
    "              .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da802d7-d682-4d8b-afe2-c05488c81ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Fundamentals of Apache Kafka\n",
    "Kafka is a distributed streaming platform designed to handle high throughput, fault-tolerant messaging in real-time. It solves the problem of ingesting, processing, and moving large-scale data streams in real time for big data applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b9fb6-6f4e-4c6f-98c0-1016de9cd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Kafka Architecture\n",
    "Producers: Send data to Kafka topics.\n",
    "Topics: Logical channels for data streams.\n",
    "Brokers: Kafka servers that store data.\n",
    "Consumers: Read data from Kafka topics.\n",
    "ZooKeeper: Manages and coordinates Kafka brokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c67063-23b5-45d2-8273-f1d6019b9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. Kafka Producer and Consumer (Python Example)\n",
    "\n",
    "Producer:\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "producer.send('my-topic', b'Hello Kafka')\n",
    "\n",
    "Consumer:\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "consumer = KafkaConsumer('my-topic', bootstrap_servers='localhost:9092')\n",
    "for message in consumer:\n",
    "    print(message.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2685f-2596-4c12-adf4-767389b74bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. Kafka Data Retention and Partitioning\n",
    "Data Retention: Configurable; data is retained for a set period or size limit, enabling fault tolerance.\n",
    "Partitioning: Kafka divides topics into partitions for scalability and parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdad74-901e-4da5-b7ce-e16f1ab147f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Real-World Kafka Use Cases\n",
    "Kafka is used in various industries for real-time analytics, log aggregation, and data pipelines. For example, LinkedIn uses Kafka for activity tracking and operational monitoring due to its scalability and fault tolerance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
